# @package training
# Those arguments defines the training hyper-parameters
epochs: 30
hypersearch: False
eval_every_x: 1
eval_corr_every_x: 101
num_workers: 8
batch_size: 8
batch_size_val: 2
optim:
  base_lr: 0.01
  weight_decay: 1e-4
  # accumulated_gradient: -1 # Accumulate gradient accumulated_gradient * batch_size
  grad_clip: -1
  optimizer:
    o_class: Adam
    params:
      lr: ${training.optim.base_lr} # The path is cut from training
  lr_scheduler: ${lr_scheduler} # CHECK THIS
just_eval: False     
multi_gpu: False
half_precision: True
gradient_checkpointing: False
wandb: False
